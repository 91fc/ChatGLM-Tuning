{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-Tuning\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting git+https://ghproxy.com/github.com/huggingface/peft.git (from -r requirements.txt (line 14))\n",
      "  Cloning https://ghproxy.com/github.com/huggingface/peft.git to /tmp/pip-req-build-s_apeqeh\n",
      "  Running command git clone --filter=blob:none --quiet https://ghproxy.com/github.com/huggingface/peft.git /tmp/pip-req-build-s_apeqeh\n",
      "  Resolved https://ghproxy.com/github.com/huggingface/peft.git to commit 098962fa6515f2e4fe83a757f5995d3ffbb1c373\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes==0.37.1\n",
      "  Using cached https://mirrors.bfsu.edu.cn/pypi/web/packages/ec/18/75dbd7529844c8600944df123160216323982d39d24a30e9f6806279f935/bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\n",
      "Requirement already satisfied: accelerate==0.17.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.17.1)\n",
      "Requirement already satisfied: protobuf<3.20.1,>=3.19.5 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.20.0)\n",
      "Collecting transformers==4.27.1\n",
      "  Using cached https://mirrors.bfsu.edu.cn/pypi/web/packages/6d/9b/2f536f9e73390209e0b27b74691355dac494b7ec8154f3012fdc6debbae7/transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "Requirement already satisfied: icetk in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.0.4)\n",
      "Requirement already satisfied: cpm_kernels==1.0.11 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.0.11)\n",
      "Requirement already satisfied: torch>=1.13.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.13.1)\n",
      "Requirement already satisfied: datasets==2.10.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (1.24.2)\n",
      "Requirement already satisfied: pyyaml in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (23.0)\n",
      "Requirement already satisfied: psutil in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (5.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (2.28.2)\n",
      "Requirement already satisfied: filelock in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 7)) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (2023.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (0.70.14)\n",
      "Requirement already satisfied: pandas in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from datasets==2.10.1->-r requirements.txt (line 13)) (11.0.0)\n",
      "Requirement already satisfied: torchvision in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from icetk->-r requirements.txt (line 8)) (0.14.1)\n",
      "Requirement already satisfied: sentencepiece in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from icetk->-r requirements.txt (line 8)) (0.1.97)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from torch>=1.13.1->-r requirements.txt (line 10)) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.1->-r requirements.txt (line 10)) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.1->-r requirements.txt (line 10)) (65.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 13)) (4.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 7)) (1.26.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 13)) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from torchvision->icetk->-r requirements.txt (line 8)) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1->-r requirements.txt (line 13)) (1.16.0)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=40691 sha256=ad645dfd1798dda9cc4f061215b4bba3e27cb24228a37adc064c156babc327b8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-950ncfec/wheels/d4/e9/68/c9cb91c478211c6c8712604dbf51cd1ad165a2336ae3c50a87\n",
      "Successfully built peft\n",
      "Installing collected packages: bitsandbytes, transformers, peft\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.1\n",
      "    Uninstalling transformers-4.26.1:\n",
      "      Successfully uninstalled transformers-4.26.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.2.0\n",
      "    Uninstalling peft-0.2.0:\n",
      "      Successfully uninstalled peft-0.2.0\n",
      "Successfully installed bitsandbytes-0.37.1 peft-0.3.0.dev0 transformers-4.27.1\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/mymusise/ChatGLM-Tuning.git\n",
    "%cd  /home/wanddy/ChatGLM-Tuning/\n",
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatting..: 100%|███████████████████| 52002/52002 [00:00<00:00, 185634.83it/s]\n"
     ]
    }
   ],
   "source": [
    "!python cover_alpaca2jsonl.py \\\n",
    "    --data_path data/alpaca_data.json \\\n",
    "    --save_path data/alpaca_data.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/wanddy/.cache/huggingface/datasets/generator/default-200c33b8652591a1/0.0.0...\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "\n",
      "Generating train split: 1 examples [04:13, 253.62s/ examples]02 [00:00<?, ?it/s]\u001b[A\n",
      "Generating train split: 169 examples [04:13,  1.05s/ examples]0:31, 1654.03it/s]\u001b[A\n",
      "Generating train split: 351 examples [04:13,  2.41 examples/s]0:29, 1743.15it/s]\u001b[A\n",
      "Generating train split: 529 examples [04:13,  4.45 examples/s]0:29, 1751.50it/s]\u001b[A\n",
      "Generating train split: 707 examples [04:14,  7.35 examples/s]0:28, 1769.23it/s]\u001b[A\n",
      "Generating train split: 921 examples [04:14, 12.31 examples/s]0:32, 1574.38it/s]\u001b[A\n",
      "Generating train split: 1099 examples [04:14, 18.15 examples/s]:33, 1519.54it/s]\u001b[A\n",
      "Generating train split: 1298 examples [04:14, 27.44 examples/s]:30, 1659.02it/s]\u001b[A\n",
      "Generating train split: 1489 examples [04:14, 40.02 examples/s]:29, 1718.77it/s]\u001b[A\n",
      "Generating train split: 1738 examples [04:14, 62.88 examples/s]:29, 1725.24it/s]\u001b[A\n",
      "Generating train split: 1919 examples [04:14, 86.05 examples/s]:29, 1711.96it/s]\u001b[A\n",
      "  4%|█▎                                  | 1953/52002 [00:01<00:29, 1722.54it/s]\u001b[A\n",
      "Generating train split: 2187 examples [04:14, 132.89 examples/s]29, 1714.47it/s]\u001b[A\n",
      "Generating train split: 2385 examples [04:15, 180.17 examples/s]27, 1801.81it/s]\u001b[A\n",
      "Generating train split: 2657 examples [04:15, 265.04 examples/s]27, 1794.15it/s]\u001b[A\n",
      "Generating train split: 2851 examples [04:15, 343.92 examples/s]27, 1809.71it/s]\u001b[A\n",
      "  6%|█▉                                  | 2878/52002 [00:01<00:27, 1814.39it/s]\u001b[A\n",
      "Generating train split: 3102 examples [04:15, 465.66 examples/s]27, 1774.54it/s]\u001b[A\n",
      "Generating train split: 3285 examples [04:15, 574.69 examples/s]27, 1792.06it/s]\u001b[A\n",
      "Generating train split: 3544 examples [04:15, 743.14 examples/s]27, 1745.13it/s]\u001b[A\n",
      "  7%|██▍                                 | 3599/52002 [00:02<00:28, 1718.92it/s]\u001b[A\n",
      "Generating train split: 3797 examples [04:15, 905.76 examples/s]27, 1752.34it/s]\u001b[A\n",
      "Generating train split: 3980 examples [04:15, 1035.33 examples/s]7, 1765.73it/s]\u001b[A\n",
      "Generating train split: 4232 examples [04:16, 1183.22 examples/s]8, 1679.16it/s]\u001b[A\n",
      "Generating train split: 4418 examples [04:16, 1303.00 examples/s]6, 1773.34it/s]\u001b[A\n",
      "Generating train split: 4601 examples [04:16, 1408.42 examples/s]6, 1776.71it/s]\u001b[A\n",
      "Generating train split: 4863 examples [04:16, 1508.55 examples/s]7, 1746.14it/s]\u001b[A\n",
      "  9%|███▍                                | 4878/52002 [00:02<00:26, 1757.53it/s]\u001b[A\n",
      "Generating train split: 5112 examples [04:16, 1552.82 examples/s]7, 1713.29it/s]\u001b[A\n",
      "Generating train split: 5290 examples [04:16, 1602.36 examples/s]7, 1728.83it/s]\u001b[A\n",
      "Generating train split: 5479 examples [04:16, 1666.61 examples/s]6, 1737.69it/s]\u001b[A\n",
      "Generating train split: 5669 examples [04:16, 1725.64 examples/s]5, 1816.88it/s]\u001b[A\n",
      "Generating train split: 5925 examples [04:17, 1709.39 examples/s]5, 1790.84it/s]\u001b[A\n",
      " 11%|████▏                               | 5972/52002 [00:03<00:26, 1745.82it/s]\u001b[A\n",
      "Generating train split: 6188 examples [04:17, 1702.56 examples/s]6, 1706.97it/s]\u001b[A\n",
      "Generating train split: 6450 examples [04:17, 1713.51 examples/s]6, 1730.83it/s]\u001b[A\n",
      "Generating train split: 6625 examples [04:17, 1720.52 examples/s]6, 1736.73it/s]\u001b[A\n",
      " 13%|████▌                               | 6677/52002 [00:03<00:26, 1704.79it/s]\u001b[A\n",
      "Generating train split: 6878 examples [04:17, 1705.68 examples/s]6, 1719.50it/s]\u001b[A\n",
      "Generating train split: 7109 examples [04:17, 1648.63 examples/s]7, 1634.46it/s]\u001b[A\n",
      "Generating train split: 7286 examples [04:17, 1674.27 examples/s]6, 1663.68it/s]\u001b[A\n",
      "Generating train split: 7533 examples [04:17, 1660.21 examples/s]6, 1688.92it/s]\u001b[A\n",
      "Generating train split: 7725 examples [04:18, 1717.44 examples/s]6, 1669.05it/s]\u001b[A\n",
      " 15%|█████▎                              | 7739/52002 [00:04<00:25, 1736.18it/s]\u001b[A\n",
      "Generating train split: 7976 examples [04:18, 1696.91 examples/s]6, 1682.62it/s]\u001b[A\n",
      "Generating train split: 8223 examples [04:18, 1676.97 examples/s]6, 1679.21it/s]\u001b[A\n",
      " 16%|█████▋                              | 8252/52002 [00:04<00:26, 1674.13it/s]\u001b[A\n",
      "Generating train split: 8478 examples [04:18, 1682.73 examples/s]6, 1666.59it/s]\u001b[A\n",
      "Generating train split: 8660 examples [04:18, 1712.64 examples/s]4, 1737.44it/s]\u001b[A\n",
      "Generating train split: 8860 examples [04:18, 1783.68 examples/s]4, 1765.03it/s]\u001b[A\n",
      "Generating train split: 9106 examples [04:18, 1731.64 examples/s]3, 1845.01it/s]\u001b[A\n",
      "Generating train split: 9290 examples [04:18, 1757.65 examples/s]4, 1718.42it/s]\u001b[A\n",
      "Generating train split: 9486 examples [04:19, 1808.96 examples/s]3, 1781.34it/s]\u001b[A\n",
      "Generating train split: 9729 examples [04:19, 1735.03 examples/s]3, 1784.42it/s]\u001b[A\n",
      " 19%|██████▋                             | 9739/52002 [00:05<00:24, 1732.97it/s]\u001b[A\n",
      "Generating train split: 9981 examples [04:19, 1711.94 examples/s]4, 1701.39it/s]\u001b[A\n",
      "Generating train split: 10238 examples [04:19, 1710.16 examples/s], 1669.99it/s]\u001b[A\n",
      " 20%|██████▉                            | 10270/52002 [00:05<00:24, 1721.28it/s]\u001b[A\n",
      "Generating train split: 10500 examples [04:19, 1719.57 examples/s], 1706.09it/s]\u001b[A\n",
      "Generating train split: 10767 examples [04:19, 1735.85 examples/s], 1738.53it/s]\u001b[A\n",
      " 21%|███████▎                           | 10804/52002 [00:06<00:23, 1752.64it/s]\u001b[A\n",
      "Generating train split: 11012 examples [04:19, 1699.15 examples/s], 1745.03it/s]\u001b[A\n",
      "Generating train split: 11251 examples [04:20, 1664.01 examples/s], 1674.38it/s]\u001b[A\n",
      "Generating train split: 11437 examples [04:20, 1702.56 examples/s], 1644.87it/s]\u001b[A\n",
      " 22%|███████▊                           | 11515/52002 [00:06<00:23, 1718.24it/s]\u001b[A\n",
      "Generating train split: 11697 examples [04:20, 1707.61 examples/s], 1701.86it/s]\u001b[A\n",
      "Generating train split: 11947 examples [04:20, 1691.83 examples/s], 1692.52it/s]\u001b[A\n",
      "Generating train split: 12186 examples [04:20, 1658.17 examples/s], 1640.60it/s]\u001b[A\n",
      "Generating train split: 12370 examples [04:20, 1697.24 examples/s], 1652.28it/s]\u001b[A\n",
      "Generating train split: 12555 examples [04:20, 1729.21 examples/s], 1715.21it/s]\u001b[A\n",
      " 24%|████████▍                          | 12566/52002 [00:07<00:22, 1741.58it/s]\u001b[A\n",
      "Generating train split: 12816 examples [04:21, 1728.17 examples/s], 1738.70it/s]\u001b[A\n",
      "Generating train split: 13053 examples [04:21, 1675.56 examples/s], 1741.42it/s]\u001b[A\n",
      "Generating train split: 13247 examples [04:21, 1737.18 examples/s], 1676.82it/s]\u001b[A\n",
      " 26%|████████▉                          | 13282/52002 [00:07<00:22, 1743.12it/s]\u001b[A\n",
      "Generating train split: 13507 examples [04:21, 1733.64 examples/s], 1722.96it/s]\u001b[A\n",
      "Generating train split: 13760 examples [04:21, 1714.01 examples/s], 1736.74it/s]\u001b[A\n",
      "Generating train split: 13939 examples [04:21, 1728.61 examples/s], 1718.42it/s]\u001b[A\n",
      " 27%|█████████▍                         | 13983/52002 [00:08<00:22, 1710.50it/s]\u001b[A\n",
      "Generating train split: 14172 examples [04:21, 1665.32 examples/s], 1640.92it/s]\u001b[A\n",
      "Generating train split: 14355 examples [04:21, 1700.91 examples/s], 1683.57it/s]\u001b[A\n",
      "Generating train split: 14542 examples [04:22, 1740.84 examples/s], 1748.97it/s]\u001b[A\n",
      "Generating train split: 14794 examples [04:22, 1714.46 examples/s], 1721.63it/s]\u001b[A\n",
      "Generating train split: 14973 examples [04:22, 1729.63 examples/s], 1722.67it/s]\u001b[A\n",
      " 29%|██████████▏                        | 15048/52002 [00:08<00:21, 1713.61it/s]\u001b[A\n",
      "Generating train split: 15222 examples [04:22, 1700.43 examples/s], 1698.88it/s]\u001b[A\n",
      "Generating train split: 15473 examples [04:22, 1686.46 examples/s], 1657.67it/s]\u001b[A\n",
      "Generating train split: 15643 examples [04:22, 1687.21 examples/s], 1666.26it/s]\u001b[A\n",
      "Generating train split: 15818 examples [04:22, 1699.67 examples/s], 1681.18it/s]\u001b[A\n",
      "Generating train split: 16055 examples [04:22, 1654.90 examples/s], 1682.01it/s]\u001b[A\n",
      "Generating train split: 16229 examples [04:23, 1674.41 examples/s], 1675.78it/s]\u001b[A\n",
      "Generating train split: 16409 examples [04:23, 1707.22 examples/s], 1688.77it/s]\u001b[A\n",
      "Generating train split: 16591 examples [04:23, 1736.57 examples/s], 1717.57it/s]\u001b[A\n",
      "Generating train split: 16781 examples [04:23, 1780.84 examples/s], 1754.53it/s]\u001b[A\n",
      " 32%|███████████▎                       | 16795/52002 [00:09<00:19, 1794.65it/s]\u001b[A\n",
      "Generating train split: 17025 examples [04:23, 1716.90 examples/s], 1763.89it/s]\u001b[A\n",
      "Generating train split: 17285 examples [04:23, 1718.66 examples/s], 1678.70it/s]\u001b[A\n",
      "Generating train split: 17460 examples [04:23, 1722.19 examples/s], 1735.09it/s]\u001b[A\n",
      "Generating train split: 17644 examples [04:23, 1750.12 examples/s], 1740.99it/s]\u001b[A\n",
      " 34%|███████████▉                       | 17694/52002 [00:10<00:19, 1752.30it/s]\u001b[A\n",
      "Generating train split: 17903 examples [04:24, 1737.91 examples/s], 1751.87it/s]\u001b[A\n",
      "Generating train split: 18148 examples [04:24, 1698.76 examples/s], 1673.85it/s]\u001b[A\n",
      "Generating train split: 18381 examples [04:24, 1648.11 examples/s], 1660.56it/s]\u001b[A\n",
      " 35%|████████████▎                      | 18382/52002 [00:10<00:20, 1632.30it/s]\u001b[A\n",
      "Generating train split: 18632 examples [04:24, 1650.85 examples/s], 1619.39it/s]\u001b[A\n",
      "Generating train split: 18812 examples [04:24, 1684.11 examples/s], 1683.15it/s]\u001b[A\n",
      "Generating train split: 18982 examples [04:24, 1686.37 examples/s], 1683.29it/s]\u001b[A\n",
      "Generating train split: 19238 examples [04:24, 1690.81 examples/s], 1664.50it/s]\u001b[A\n",
      " 37%|████████████▉                      | 19250/52002 [00:11<00:19, 1708.71it/s]\u001b[A\n",
      "Generating train split: 19494 examples [04:24, 1685.02 examples/s], 1674.84it/s]\u001b[A\n",
      "Generating train split: 19752 examples [04:25, 1692.25 examples/s], 1695.26it/s]\u001b[A\n",
      "Generating train split: 19926 examples [04:25, 1698.38 examples/s], 1708.69it/s]\u001b[A\n",
      "Generating train split: 20097 examples [04:25, 1698.96 examples/s], 1713.22it/s]\u001b[A\n",
      "Generating train split: 20271 examples [04:25, 1708.52 examples/s], 1675.98it/s]\u001b[A\n",
      "Generating train split: 20449 examples [04:25, 1726.45 examples/s], 1731.45it/s]\u001b[A\n",
      " 39%|█████████████▊                     | 20478/52002 [00:11<00:18, 1733.91it/s]\u001b[A\n",
      "Generating train split: 20717 examples [04:25, 1746.34 examples/s], 1721.49it/s]\u001b[A\n",
      "Generating train split: 20907 examples [04:25, 1782.92 examples/s], 1810.47it/s]\u001b[A\n",
      "Generating train split: 21107 examples [04:25, 1795.18 examples/s], 1770.47it/s]\u001b[A\n",
      "Generating train split: 21290 examples [04:25, 1803.55 examples/s], 1809.79it/s]\u001b[A\n",
      "Generating train split: 21549 examples [04:26, 1773.21 examples/s], 1782.63it/s]\u001b[A\n",
      "Generating train split: 21731 examples [04:26, 1782.03 examples/s], 1775.93it/s]\u001b[A\n",
      " 42%|██████████████▋                    | 21769/52002 [00:12<00:16, 1782.27it/s]\u001b[A\n",
      "Generating train split: 21991 examples [04:26, 1762.75 examples/s], 1777.87it/s]\u001b[A\n",
      "Generating train split: 22241 examples [04:26, 1725.55 examples/s], 1730.77it/s]\u001b[A\n",
      " 43%|███████████████                    | 22300/52002 [00:12<00:17, 1728.27it/s]\u001b[A\n",
      "Generating train split: 22500 examples [04:26, 1721.37 examples/s], 1737.51it/s]\u001b[A\n",
      "Generating train split: 22675 examples [04:26, 1725.20 examples/s], 1712.26it/s]\u001b[A\n",
      "Generating train split: 22929 examples [04:26, 1711.81 examples/s], 1710.11it/s]\u001b[A\n",
      " 44%|███████████████▍                   | 22996/52002 [00:13<00:16, 1715.00it/s]\u001b[A\n",
      "Generating train split: 23173 examples [04:27, 1673.06 examples/s], 1654.78it/s]\u001b[A\n",
      "Generating train split: 23359 examples [04:27, 1714.73 examples/s], 1707.10it/s]\u001b[A\n",
      "Generating train split: 23540 examples [04:27, 1735.39 examples/s], 1741.64it/s]\u001b[A\n",
      "Generating train split: 23725 examples [04:27, 1762.34 examples/s], 1769.53it/s]\u001b[A\n",
      "Generating train split: 23996 examples [04:27, 1771.51 examples/s], 1758.14it/s]\u001b[A\n",
      "Generating train split: 24197 examples [04:27, 1624.43 examples/s], 1658.99it/s]\u001b[A\n",
      "Generating train split: 24371 examples [04:27, 1650.07 examples/s], 1611.78it/s]\u001b[A\n",
      " 47%|████████████████▍                  | 24417/52002 [00:14<00:16, 1648.96it/s]\u001b[A\n",
      "Generating train split: 24618 examples [04:27, 1642.82 examples/s], 1667.80it/s]\u001b[A\n",
      "Generating train split: 24784 examples [04:28, 1642.81 examples/s], 1651.16it/s]\u001b[A\n",
      "Generating train split: 25004 examples [04:28, 1579.43 examples/s], 1629.74it/s]\u001b[A\n",
      "Generating train split: 25169 examples [04:28, 1594.56 examples/s], 1560.48it/s]\u001b[A\n",
      "Generating train split: 25406 examples [04:28, 1587.80 examples/s], 1595.63it/s]\u001b[A\n",
      "Generating train split: 25590 examples [04:28, 1645.41 examples/s], 1575.80it/s]\u001b[A\n",
      " 49%|█████████████████▏                 | 25599/52002 [00:14<00:16, 1645.39it/s]\u001b[A\n",
      "Generating train split: 25832 examples [04:28, 1632.55 examples/s], 1621.87it/s]\u001b[A\n",
      "Generating train split: 26064 examples [04:28, 1600.13 examples/s], 1590.62it/s]\u001b[A\n",
      "Generating train split: 26237 examples [04:28, 1630.87 examples/s], 1615.14it/s]\u001b[A\n",
      "Generating train split: 26407 examples [04:29, 1646.29 examples/s], 1656.15it/s]\u001b[A\n",
      "Generating train split: 26589 examples [04:29, 1688.23 examples/s], 1632.97it/s]\u001b[A\n",
      " 51%|█████████████████▉                 | 26623/52002 [00:15<00:15, 1690.09it/s]\u001b[A\n",
      "Generating train split: 26824 examples [04:29, 1639.46 examples/s], 1634.73it/s]\u001b[A\n",
      "Generating train split: 27000 examples [04:29, 1621.89 examples/s], 1674.89it/s]\u001b[A\n",
      "Generating train split: 27182 examples [04:29, 1670.09 examples/s], 1647.18it/s]\u001b[A\n",
      "Generating train split: 27355 examples [04:29, 1683.29 examples/s], 1681.83it/s]\u001b[A\n",
      "Generating train split: 27540 examples [04:29, 1726.84 examples/s], 1730.44it/s]\u001b[A\n",
      "Generating train split: 27784 examples [04:29, 1683.70 examples/s], 1699.51it/s]\u001b[A\n",
      "Generating train split: 28010 examples [04:30, 1619.71 examples/s], 1666.18it/s]\u001b[A\n",
      " 54%|██████████████████▊                | 28014/52002 [00:16<00:14, 1601.25it/s]\u001b[A\n",
      "Generating train split: 28254 examples [04:30, 1620.27 examples/s], 1611.92it/s]\u001b[A\n",
      "Generating train split: 28422 examples [04:30, 1629.60 examples/s], 1613.41it/s]\u001b[A\n",
      "Generating train split: 28669 examples [04:30, 1633.36 examples/s], 1641.41it/s]\u001b[A\n",
      "Generating train split: 28862 examples [04:30, 1702.51 examples/s], 1628.07it/s]\u001b[A\n",
      " 56%|███████████████████▍               | 28867/52002 [00:16<00:13, 1708.32it/s]\u001b[A\n",
      "Generating train split: 29108 examples [04:30, 1673.86 examples/s], 1671.64it/s]\u001b[A\n",
      "Generating train split: 29284 examples [04:30, 1693.01 examples/s], 1685.18it/s]\u001b[A\n",
      "Generating train split: 29463 examples [04:30, 1716.02 examples/s], 1703.33it/s]\u001b[A\n",
      "Generating train split: 29722 examples [04:31, 1717.93 examples/s], 1699.25it/s]\u001b[A\n",
      " 57%|████████████████████               | 29736/52002 [00:17<00:12, 1724.20it/s]\u001b[A\n",
      "Generating train split: 29976 examples [04:31, 1703.28 examples/s], 1706.24it/s]\u001b[A\n",
      "Generating train split: 30211 examples [04:31, 1653.17 examples/s], 1655.64it/s]\u001b[A\n",
      "Generating train split: 30396 examples [04:31, 1697.81 examples/s], 1679.99it/s]\u001b[A\n",
      " 59%|████████████████████▍              | 30429/52002 [00:17<00:12, 1699.97it/s]\u001b[A\n",
      "Generating train split: 30628 examples [04:31, 1642.28 examples/s], 1624.24it/s]\u001b[A\n",
      "Generating train split: 30808 examples [04:31, 1678.82 examples/s], 1683.30it/s]\u001b[A\n",
      "Generating train split: 30987 examples [04:31, 1702.49 examples/s], 1711.33it/s]\u001b[A\n",
      "Generating train split: 31223 examples [04:31, 1649.33 examples/s], 1665.05it/s]\u001b[A\n",
      "Generating train split: 31402 examples [04:32, 1680.58 examples/s], 1677.65it/s]\u001b[A\n",
      " 61%|█████████████████████▏             | 31477/52002 [00:18<00:12, 1688.35it/s]\u001b[A\n",
      "Generating train split: 31670 examples [04:32, 1712.99 examples/s], 1716.42it/s]\u001b[A\n",
      "Generating train split: 31871 examples [04:32, 1782.02 examples/s], 1790.53it/s]\u001b[A\n",
      "Generating train split: 32129 examples [04:32, 1758.22 examples/s], 1783.94it/s]\u001b[A\n",
      "Generating train split: 32379 examples [04:32, 1724.18 examples/s], 1767.44it/s]\u001b[A\n",
      "Generating train split: 32564 examples [04:32, 1751.67 examples/s], 1713.28it/s]\u001b[A\n",
      " 63%|█████████████████████▉             | 32575/52002 [00:19<00:11, 1753.43it/s]\u001b[A\n",
      "Generating train split: 32826 examples [04:32, 1743.95 examples/s], 1719.38it/s]\u001b[A\n",
      "Generating train split: 33097 examples [04:33, 1731.37 examples/s], 1749.11it/s]\u001b[A\n",
      " 64%|██████████████████████▎            | 33110/52002 [00:19<00:10, 1730.16it/s]\u001b[A\n",
      "Generating train split: 33358 examples [04:33, 1729.08 examples/s], 1725.67it/s]\u001b[A\n",
      "Generating train split: 33535 examples [04:33, 1735.28 examples/s], 1728.31it/s]\u001b[A\n",
      "Generating train split: 33719 examples [04:33, 1759.38 examples/s], 1778.35it/s]\u001b[A\n",
      "Generating train split: 33979 examples [04:33, 1748.01 examples/s], 1753.34it/s]\u001b[A\n",
      " 65%|██████████████████████▉            | 34003/52002 [00:19<00:10, 1696.13it/s]\u001b[A\n",
      "Generating train split: 34221 examples [04:33, 1699.82 examples/s], 1692.44it/s]\u001b[A\n",
      "Generating train split: 34395 examples [04:33, 1707.35 examples/s], 1694.38it/s]\u001b[A\n",
      "Generating train split: 34653 examples [04:33, 1709.65 examples/s], 1713.58it/s]\u001b[A\n",
      "Generating train split: 34827 examples [04:34, 1715.61 examples/s], 1728.51it/s]\u001b[A\n",
      "Generating train split: 35000 examples [04:34, 1668.11 examples/s], 1718.36it/s]\u001b[A\n",
      "Generating train split: 35174 examples [04:34, 1685.42 examples/s], 1669.87it/s]\u001b[A\n",
      "Generating train split: 35348 examples [04:34, 1696.10 examples/s], 1700.30it/s]\u001b[A\n",
      "Generating train split: 35531 examples [04:34, 1731.92 examples/s], 1689.33it/s]\u001b[A\n",
      "Generating train split: 35709 examples [04:34, 1743.48 examples/s], 1728.83it/s]\u001b[A\n",
      " 69%|████████████████████████           | 35755/52002 [00:20<00:09, 1743.10it/s]\u001b[A\n",
      "Generating train split: 35948 examples [04:34, 1681.81 examples/s], 1689.00it/s]\u001b[A\n",
      "Generating train split: 36175 examples [04:34, 1619.70 examples/s], 1580.63it/s]\u001b[A\n",
      "Generating train split: 36339 examples [04:34, 1620.12 examples/s], 1602.71it/s]\u001b[A\n",
      "Generating train split: 36504 examples [04:35, 1625.64 examples/s], 1627.96it/s]\u001b[A\n",
      "Generating train split: 36691 examples [04:35, 1691.04 examples/s], 1691.09it/s]\u001b[A\n",
      "Generating train split: 36885 examples [04:35, 1757.93 examples/s], 1722.67it/s]\u001b[A\n",
      "Generating train split: 37141 examples [04:35, 1736.39 examples/s], 1780.62it/s]\u001b[A\n",
      "Generating train split: 37327 examples [04:35, 1763.90 examples/s], 1747.54it/s]\u001b[A\n",
      " 72%|█████████████████████████▏         | 37355/52002 [00:21<00:08, 1766.83it/s]\u001b[A\n",
      "Generating train split: 37573 examples [04:35, 1715.36 examples/s], 1742.82it/s]\u001b[A\n",
      "Generating train split: 37757 examples [04:35, 1746.92 examples/s], 1733.22it/s]\u001b[A\n",
      "Generating train split: 37936 examples [04:35, 1757.59 examples/s], 1743.23it/s]\u001b[A\n",
      "Generating train split: 38194 examples [04:35, 1737.82 examples/s], 1688.65it/s]\u001b[A\n",
      " 74%|█████████████████████████▋         | 38239/52002 [00:22<00:08, 1716.27it/s]\u001b[A\n",
      "Generating train split: 38448 examples [04:36, 1720.35 examples/s], 1719.59it/s]\u001b[A\n",
      "Generating train split: 38627 examples [04:36, 1729.03 examples/s], 1732.80it/s]\u001b[A\n",
      "Generating train split: 38874 examples [04:36, 1696.61 examples/s], 1724.44it/s]\u001b[A\n",
      " 75%|██████████████████████████▏        | 38936/52002 [00:22<00:07, 1714.13it/s]\u001b[A\n",
      "Generating train split: 39121 examples [04:36, 1676.08 examples/s], 1656.63it/s]\u001b[A\n",
      "Generating train split: 39293 examples [04:36, 1685.71 examples/s], 1673.93it/s]\u001b[A\n",
      "Generating train split: 39473 examples [04:36, 1714.16 examples/s], 1714.15it/s]\u001b[A\n",
      "Generating train split: 39728 examples [04:36, 1708.73 examples/s], 1713.29it/s]\u001b[A\n",
      "Generating train split: 39904 examples [04:36, 1718.96 examples/s], 1732.77it/s]\u001b[A\n",
      "Generating train split: 40150 examples [04:37, 1688.19 examples/s], 1736.33it/s]\u001b[A\n",
      "Generating train split: 40331 examples [04:37, 1714.31 examples/s], 1684.58it/s]\u001b[A\n",
      " 78%|███████████████████████████▏       | 40336/52002 [00:23<00:06, 1699.45it/s]\u001b[A\n",
      "Generating train split: 40589 examples [04:37, 1712.65 examples/s], 1677.96it/s]\u001b[A\n",
      "Generating train split: 40775 examples [04:37, 1744.95 examples/s], 1747.49it/s]\u001b[A\n",
      "Generating train split: 40964 examples [04:37, 1779.80 examples/s], 1764.61it/s]\u001b[A\n",
      "Generating train split: 41209 examples [04:37, 1724.17 examples/s], 1740.46it/s]\u001b[A\n",
      " 79%|███████████████████████████▊       | 41235/52002 [00:24<00:06, 1751.55it/s]\u001b[A\n",
      "Generating train split: 41466 examples [04:37, 1715.66 examples/s], 1719.87it/s]\u001b[A\n",
      "Generating train split: 41640 examples [04:38, 1715.59 examples/s], 1741.77it/s]\u001b[A\n",
      "Generating train split: 41871 examples [04:38, 1652.84 examples/s], 1657.43it/s]\u001b[A\n",
      "Generating train split: 42098 examples [04:38, 1602.75 examples/s], 1644.21it/s]\u001b[A\n",
      "Generating train split: 42269 examples [04:38, 1624.86 examples/s], 1576.24it/s]\u001b[A\n",
      "Generating train split: 42436 examples [04:38, 1634.14 examples/s], 1612.61it/s]\u001b[A\n",
      "Generating train split: 42627 examples [04:38, 1704.56 examples/s], 1625.58it/s]\u001b[A\n",
      " 82%|████████████████████████████▋      | 42627/52002 [00:24<00:05, 1705.31it/s]\u001b[A\n",
      "Generating train split: 42860 examples [04:38, 1648.72 examples/s], 1653.35it/s]\u001b[A\n",
      "Generating train split: 43101 examples [04:38, 1631.27 examples/s], 1654.46it/s]\u001b[A\n",
      " 83%|█████████████████████████████      | 43132/52002 [00:25<00:05, 1637.51it/s]\u001b[A\n",
      "Generating train split: 43335 examples [04:39, 1600.11 examples/s], 1607.91it/s]\u001b[A\n",
      "Generating train split: 43500 examples [04:39, 1609.10 examples/s], 1581.03it/s]\u001b[A\n",
      "Generating train split: 43674 examples [04:39, 1641.05 examples/s], 1625.49it/s]\u001b[A\n",
      "Generating train split: 43848 examples [04:39, 1665.25 examples/s], 1660.40it/s]\u001b[A\n",
      "Generating train split: 44080 examples [04:39, 1614.17 examples/s], 1678.91it/s]\u001b[A\n",
      "Generating train split: 44249 examples [04:39, 1632.72 examples/s], 1589.08it/s]\u001b[A\n",
      "Generating train split: 44424 examples [04:39, 1659.35 examples/s], 1654.51it/s]\u001b[A\n",
      "Generating train split: 44610 examples [04:39, 1712.30 examples/s], 1667.37it/s]\u001b[A\n",
      "Generating train split: 44792 examples [04:39, 1741.55 examples/s], 1714.40it/s]\u001b[A\n",
      "Generating train split: 44968 examples [04:40, 1745.43 examples/s], 1735.77it/s]\u001b[A\n",
      "Generating train split: 45207 examples [04:40, 1680.10 examples/s], 1691.47it/s]\u001b[A\n",
      "Generating train split: 45388 examples [04:40, 1711.17 examples/s], 1690.64it/s]\u001b[A\n",
      "Generating train split: 45563 examples [04:40, 1718.73 examples/s], 1720.59it/s]\u001b[A\n",
      "Generating train split: 45756 examples [04:40, 1775.60 examples/s], 1724.87it/s]\u001b[A\n",
      " 88%|██████████████████████████████▊    | 45757/52002 [00:26<00:03, 1782.59it/s]\u001b[A\n",
      "Generating train split: 46000 examples [04:40, 1687.53 examples/s], 1754.41it/s]\u001b[A\n",
      "Generating train split: 46243 examples [04:40, 1662.59 examples/s], 1640.97it/s]\u001b[A\n",
      "Generating train split: 46418 examples [04:40, 1683.19 examples/s], 1651.80it/s]\u001b[A\n",
      "Generating train split: 46591 examples [04:40, 1693.45 examples/s], 1703.56it/s]\u001b[A\n",
      "Generating train split: 46773 examples [04:41, 1723.00 examples/s], 1698.51it/s]\u001b[A\n",
      "Generating train split: 46957 examples [04:41, 1751.17 examples/s], 1725.22it/s]\u001b[A\n",
      " 90%|███████████████████████████████▋   | 46996/52002 [00:27<00:02, 1747.47it/s]\u001b[A\n",
      "Generating train split: 47200 examples [04:41, 1698.92 examples/s], 1708.50it/s]\u001b[A\n",
      "Generating train split: 47372 examples [04:41, 1702.75 examples/s], 1701.65it/s]\u001b[A\n",
      "Generating train split: 47625 examples [04:41, 1692.02 examples/s], 1684.65it/s]\u001b[A\n",
      "Generating train split: 47801 examples [04:41, 1704.80 examples/s], 1707.24it/s]\u001b[A\n",
      "Generating train split: 48026 examples [04:41, 1629.83 examples/s], 1660.18it/s]\u001b[A\n",
      "Generating train split: 48200 examples [04:41, 1656.33 examples/s], 1620.34it/s]\u001b[A\n",
      "Generating train split: 48372 examples [04:42, 1669.94 examples/s], 1651.15it/s]\u001b[A\n",
      "Generating train split: 48546 examples [04:42, 1687.77 examples/s], 1668.24it/s]\u001b[A\n",
      "Generating train split: 48719 examples [04:42, 1697.32 examples/s], 1687.14it/s]\u001b[A\n",
      "Generating train split: 48894 examples [04:42, 1708.33 examples/s], 1694.05it/s]\u001b[A\n",
      " 94%|████████████████████████████████▉  | 48898/52002 [00:28<00:01, 1712.18it/s]\u001b[A\n",
      "Generating train split: 49134 examples [04:42, 1662.34 examples/s], 1674.35it/s]\u001b[A\n",
      "Generating train split: 49317 examples [04:42, 1704.36 examples/s], 1691.30it/s]\u001b[A\n",
      "Generating train split: 49556 examples [04:42, 1661.26 examples/s], 1704.23it/s]\u001b[A\n",
      "Generating train split: 49736 examples [04:42, 1693.69 examples/s], 1664.66it/s]\u001b[A\n",
      "Generating train split: 49912 examples [04:42, 1704.64 examples/s], 1716.53it/s]\u001b[A\n",
      " 96%|█████████████████████████████████▌ | 49948/52002 [00:29<00:01, 1691.91it/s]\u001b[A\n",
      "Generating train split: 50138 examples [04:43, 1628.55 examples/s], 1611.69it/s]\u001b[A\n",
      "Generating train split: 50316 examples [04:43, 1661.88 examples/s], 1662.71it/s]\u001b[A\n",
      "Generating train split: 50566 examples [04:43, 1660.09 examples/s], 1618.61it/s]\u001b[A\n",
      " 97%|██████████████████████████████████ | 50636/52002 [00:29<00:00, 1641.14it/s]\u001b[A\n",
      "Generating train split: 50812 examples [04:43, 1651.73 examples/s], 1654.23it/s]\u001b[A\n",
      "Generating train split: 50991 examples [04:43, 1683.15 examples/s], 1702.25it/s]\u001b[A\n",
      "Generating train split: 51249 examples [04:43, 1689.82 examples/s], 1659.35it/s]\u001b[A\n",
      "Generating train split: 51502 examples [04:43, 1683.44 examples/s], 1716.96it/s]\u001b[A\n",
      "Generating train split: 51688 examples [04:44, 1721.85 examples/s], 1694.42it/s]\u001b[A\n",
      "Generating train split: 51863 examples [04:44, 1725.55 examples/s], 1727.91it/s]\u001b[A\n",
      "100%|███████████████████████████████████| 52002/52002 [00:30<00:00, 1700.20it/s]\u001b[A\n",
      "Dataset generator downloaded and prepared to /home/wanddy/.cache/huggingface/datasets/generator/default-200c33b8652591a1/0.0.0. Subsequent calls will reuse this data.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python tokenize_dataset_rows.py \\\n",
    "    --jsonl_path data/alpaca_data.jsonl \\\n",
    "    --save_path data/alpaca \\\n",
    "    --max_seq_length 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Downloading shards: 100%|██████████| 8/8 [00:13<00:00,  1.63s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "from modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "model = ChatGLMForConditionalGeneration.from_pretrained(\"THUDM/chatglm-6b\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test before finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-Tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Answer: I'm sorry, but I'm not sure what you're asking. Could you please provide more context or clarify your question?\n",
      "### 1.Answer:\n",
      " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule. \n",
      "\n",
      "\n",
      "Instruction: What are the three primary colors?\n",
      "Answer: The three primary colors in painting are red, blue, and green. These colors are used to create a range of other colors, including yellow, orange, and pink. The use of these colors in painting is a key aspect of the art form.\n",
      "### 2.Answer:\n",
      " The three primary colors are red, blue, and yellow. \n",
      "\n",
      "\n",
      "Instruction: Describe the structure of an atom.\n",
      "Answer: The原子是构成物质的基本单位,由带正电荷的质子、带负电荷的电子和中间由电子围绕的原子核组成。原子核由质子和中子组成,中子在原子核内部运动,质子则围绕原子核旋转。\n",
      "\n",
      "原子通过化学反应与其他物质发生相互作用。化学反应是由原子之间的相互作用引起的,因此了解原子的结构和相互作用是理解化学和物质世界的基础。\n",
      "\n",
      "原子的大小通常用其半径来表示,半径越小,原子越紧密,其化学性质也越不稳定。原子的大小取决于原子的质子数和中子数,以及原子所处的化学环境。\n",
      "\n",
      "原子核的大小\n",
      "### 3.Answer:\n",
      " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
      "\n",
      "\n",
      "Instruction: How can we reduce air pollution?\n",
      "Answer: I'm sorry, but I'm not sure what you're asking. Could you please provide more context or clarify your question?\n",
      "### 4.Answer:\n",
      " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
      "\n",
      "\n",
      "Instruction: Describe a time when you had to make a difficult decision.\n",
      "Answer: I'm sorry, but I'm not sure what you're asking for. Could you please provide more context or clarify your question?\n",
      "### 5.Answer:\n",
      " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd  /home/wanddy/ChatGLM-Tuning/\n",
    "from cover_alpaca2jsonl import format_example\n",
    "import json\n",
    "\n",
    "\n",
    "instructions = json.load(open(\"data/alpaca_data.json\"))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature[\"context\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0])\n",
    "        print(answer)\n",
    "        item['infer_answer'] = answer\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_path = \"data/alpaca/\"\n",
    "\n",
    "dataset = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "train_num = 500\n",
    "\n",
    "mini_train_dataset = datasets.Dataset.from_dict(dataset[:train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, HfArgumentParser\n",
    "\n",
    "\n",
    "def get_masks_and_position_ids(\n",
    "    seq, seq_len, context_length, device, gmask=False, position_encoding_2d=True\n",
    "):\n",
    "    mask_position = (\n",
    "        seq_len - 2\n",
    "    )  # is equal to `seq.index(mask_token)` or `seq.index(150001)`\n",
    "    attention_mask = torch.ones((1, context_length, context_length), device=device)\n",
    "    attention_mask.tril_()\n",
    "    attention_mask[..., : mask_position - 1] = 1\n",
    "    attention_mask = (attention_mask < 0.5).bool()\n",
    "\n",
    "    if position_encoding_2d:\n",
    "        seq_length = seq_len - 1  # is equal to `seq_length = seq.index(150004)`\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[seq_length:] = mask_position\n",
    "        block_position_ids = torch.cat(\n",
    "            (\n",
    "                torch.zeros(seq_length, dtype=torch.long, device=device),\n",
    "                torch.arange(\n",
    "                    context_length - seq_length, dtype=torch.long, device=device\n",
    "                )\n",
    "                + 1,\n",
    "            )\n",
    "        )\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
    "    else:\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[context_length - 1 :] = mask_position\n",
    "    return attention_mask, position_ids\n",
    "\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    attention_mask_list = []\n",
    "    position_ids_list = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1)\n",
    "            + ids[(seq_len - 1) :]\n",
    "            + [-100] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        attention_mask, position_ids = get_masks_and_position_ids(\n",
    "            ids, seq_len, longest, _ids.device, gmask=False\n",
    "        )\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        position_ids_list.append(position_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    attention_mask = torch.stack(attention_mask_list)\n",
    "    position_ids = torch.stack(position_ids_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"position_ids\": position_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "class ModifiedTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            position_ids=inputs[\"position_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss\n",
    "    \n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        from transformers.trainer import TRAINING_ARGS_NAME\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "        saved_params = {\n",
    "            k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
    "        }\n",
    "        torch.save(saved_params, os.path.join(output_dir, \"adapter_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s]/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 50/1500 [00:56<24:55,  1.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9924, 'learning_rate': 9.713333333333334e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 100/1500 [01:51<27:17,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2892, 'learning_rate': 9.38e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 150/1500 [02:46<24:34,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9502, 'learning_rate': 9.046666666666667e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 200/1500 [03:41<22:53,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9491, 'learning_rate': 8.713333333333333e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 250/1500 [04:38<25:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9545, 'learning_rate': 8.38e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 300/1500 [05:33<20:45,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8294, 'learning_rate': 8.046666666666667e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 350/1500 [06:29<21:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7941, 'learning_rate': 7.713333333333333e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 400/1500 [07:23<19:41,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6172, 'learning_rate': 7.38e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 450/1500 [08:20<18:41,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5003, 'learning_rate': 7.046666666666667e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 500/1500 [09:16<19:27,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7071, 'learning_rate': 6.713333333333334e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 37%|███▋      | 550/1500 [10:28<19:06,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5488, 'learning_rate': 6.38e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 600/1500 [11:24<17:13,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4944, 'learning_rate': 6.046666666666667e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 650/1500 [12:19<15:05,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4185, 'learning_rate': 5.713333333333334e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 700/1500 [13:14<14:22,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5353, 'learning_rate': 5.380000000000001e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 750/1500 [14:10<14:05,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.309, 'learning_rate': 5.0466666666666676e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 800/1500 [15:06<13:57,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.66, 'learning_rate': 4.713333333333333e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 850/1500 [16:01<12:23,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5114, 'learning_rate': 4.38e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 900/1500 [16:56<12:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5485, 'learning_rate': 4.046666666666667e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 950/1500 [17:51<10:33,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5602, 'learning_rate': 3.713333333333334e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1000/1500 [18:47<08:50,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5617, 'learning_rate': 3.38e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 70%|███████   | 1050/1500 [19:57<08:03,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4773, 'learning_rate': 3.0466666666666664e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1100/1500 [20:53<07:16,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2624, 'learning_rate': 2.7133333333333333e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1150/1500 [21:48<06:06,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3397, 'learning_rate': 2.38e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1200/1500 [22:44<05:51,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2605, 'learning_rate': 2.046666666666667e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1250/1500 [23:38<04:32,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1593, 'learning_rate': 1.7133333333333334e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1300/1500 [24:34<04:02,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1815, 'learning_rate': 1.3800000000000002e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1350/1500 [25:29<02:39,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.333, 'learning_rate': 1.0466666666666668e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1400/1500 [26:25<01:57,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2108, 'learning_rate': 7.133333333333333e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1450/1500 [27:23<00:59,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4357, 'learning_rate': 3.8e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [28:19<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3097, 'learning_rate': 4.666666666666667e-07, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [28:31<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1711.9855, 'train_samples_per_second': 0.876, 'train_steps_per_second': 0.876, 'train_loss': 1.5900354690551757, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=1.5900354690551757, metrics={'train_runtime': 1711.9855, 'train_samples_per_second': 0.876, 'train_steps_per_second': 0.876, 'train_loss': 1.5900354690551757, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"output\",\n",
    "    fp16 =True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    learning_rate = 1e-4,\n",
    "    max_steps=1500,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    seed=0,\n",
    "    data_seed=0,\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    train_dataset=mini_train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "# save model\n",
    "model.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test After finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Answer: 1. Eat a balanced diet.\n",
      "2. Get regular exercise.\n",
      "3. Stay hydrated.\n",
      "### 1.Answer:\n",
      " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule. \n",
      "\n",
      "\n",
      "Instruction: What are the three primary colors?\n",
      "Answer: The three primary colors are red, blue, and yellow.\n",
      "### 2.Answer:\n",
      " The three primary colors are red, blue, and yellow. \n",
      "\n",
      "\n",
      "Instruction: Describe the structure of an atom.\n",
      "Answer: An atom is made up of a small number of particles called electrons, protons, and neutrons. Electrons are the most important part of an atom, as they are responsible for the properties of an atom. Protons and neutrons are also important, as they are responsible for the properties of an atom. Electrons are responsible for the properties of an atom, such as its charge and its ability to attract or repel each other. Protons and neutrons are responsible for the properties of an atom, such as its mass and its ability to attract or repel each other.\n",
      "### 3.Answer:\n",
      " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
      "\n",
      "\n",
      "Instruction: How can we reduce air pollution?\n",
      "Answer: There are several ways to reduce air pollution, including:\n",
      "\n",
      "1. Reduce energy consumption: Reduce energy consumption by reducing energy efficiency, reducing the use of energy-efficient appliances, and reducing the use of energy-efficient lighting.\n",
      "\n",
      "2. Use energy-efficient appliances: Use energy-efficient appliances such as LED lights, LED lights, and energy-efficient appliances.\n",
      "\n",
      "3. Use energy-efficient lighting: Use energy-efficient lighting such as LED lights and energy-efficient lights.\n",
      "\n",
      "4. Use efficient appliances: Use efficient appliances such as LED lights, LED lights, and energy-efficient appliances.\n",
      "\n",
      "5. Use public transportation: Use\n",
      "### 4.Answer:\n",
      " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
      "\n",
      "\n",
      "Instruction: Describe a time when you had to make a difficult decision.\n",
      "Answer: When I had to make a difficult decision, I was presented with a choice between two options that were both equally important. I had to decide which option was more important to me and which one I should prioritize. I had to make a decision that was based on my values and what was most important to me.\n",
      "### 5.Answer:\n",
      " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cover_alpaca2jsonl import format_example\n",
    "import json\n",
    "\n",
    "\n",
    "instructions = json.load(open(\"data/alpaca_data.json\"))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature[\"context\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0])\n",
    "        print(answer)\n",
    "        item['infer_answer'] = answer\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def save_tunable_parameters(model, path):\n",
    "    saved_params = {\n",
    "        k: v.to(\"cpu\")\n",
    "        for k, v in model.named_parameters()\n",
    "        if v.requires_grad\n",
    "    }\n",
    "    torch.save(saved_params, path)\n",
    "\n",
    "\n",
    "save_tunable_parameters(model, os.path.join(\"output\", \"chatglm-lora.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
