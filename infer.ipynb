{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "from modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = ChatGLMForConditionalGeneration.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/wanddy/ChatGLM-6B/.conda/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/peft/tuners/lora.py:175: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_path = \"output/adapter_model.bin\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=True,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instructions = json.load(open(\"data/alpaca_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanddy/ChatGLM-6B/.conda/lib/python3.9/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Answer: 1. Eat a balanced diet. \n",
      "2. Get regular exercise. \n",
      "3. Stay hydrated.\n",
      "### 1.Answer:\n",
      " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule. \n",
      "\n",
      "\n",
      "Instruction: What are the three primary colors?\n",
      "Answer: The three primary colors are red, blue, and yellow.\n",
      "### 2.Answer:\n",
      " The three primary colors are red, blue, and yellow. \n",
      "\n",
      "\n",
      "Instruction: Describe the structure of an atom.\n",
      "Answer: An atom is made up of a small number of particles, which are called electrons, protons, and neutrons. Electrons are the most important part of an atom, as they are responsible for the chemical properties of the material. Protons and neutrons are also important, as they are responsible for the structure of the atom and the properties of the material. Protons are located at the center of the atom, while neutrons are located at the center of the electron cloud. Electrons are located on the outside of the atom, and they are responsible for the chemical properties of the material.\n",
      "### 3.Answer:\n",
      " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
      "\n",
      "\n",
      "Instruction: How can we reduce air pollution?\n",
      "Answer: There are several ways to reduce air pollution, including reducing the use of plastic products, reducing the use of single-use plastics, reducing the use of energy-efficient appliances, reducing the use of pesticides, reducing the use of pesticides in the environment, reducing the use of pesticides in food production, reducing the use of pesticides in the environment, reducing the use of pesticides in food production, reducing the use of pesticides in the environment, reducing the use of pesticides in food production, reducing the use of pesticides in the environment, reducing the use of pesticides in food production, reducing the use of pesticides in the environment, reducing the use of pesticides in food production, reducing the use of pesticides\n",
      "### 4.Answer:\n",
      " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
      "\n",
      "\n",
      "Instruction: Describe a time when you had to make a difficult decision.\n",
      "Answer: When I had to make a difficult decision, I was faced with a choice between two options that were both equally important. I knew that I had to make a decision, but I was unsure which one was the better option. I had to decide between a career in which I was passionate and a career that was more stable and secure. I had to make a decision based on my own values and beliefs. I had to make a decision that was right for me, and I had to make the decision that was right for the organization.\n",
      "### 5.Answer:\n",
      " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "from cover_alpaca2jsonl import format_example\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature['context']\n",
    "        ids = tokenizer.encode(input_text)\n",
    "        input_ids = torch.LongTensor([ids])\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            do_sample=False,\n",
    "            temperature=0\n",
    "        )\n",
    "        out_text = tokenizer.decode(out[0])\n",
    "        answer = out_text.replace(input_text, \"\").replace(\"\\nEND\", \"\").strip()\n",
    "        item['infer_answer'] = answer\n",
    "        print(out_text)\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')\n",
    "        answers.append({'index': idx, **item})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
